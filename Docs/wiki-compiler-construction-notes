http://en.wikipedia.org/wiki/Book:Compiler_construction

### --- Intro ---

### 0 Compiler construction: Intro

So, we're dealing with syntax, how we use characters to represent an 'element',
grammar, the ways in which we are allowed to combine elements to make valid
combinations, and semantics, the expression of what we want to achieve,
using the combination of the aforementions elements.

### 1 Compiler

A compiler simple converts one language to another.  It determines the
correctness of the syntax, generates correct and efficient object code,
performs run-time organisation and formats the output according to assembler
and/or linker conventions--whatever all that exactly means.

It's divided up into the front, middle and back end.

Front: It performs code substitutions, macros in C, for example.  This is
called preprocessing.  It breaks up the source code into small 'tokens',
such as keywords and variables.  This is called lexical analysis.  Using the
language's grammar, the tokens are manipulated into a 'parse tree' a repre-
sentation of the program's instructions.  It then checks the parse tree
for type errors, associates variables and functions to their defintions
(generating the symbol table?), rejecting incorrect assignments.  This is
called semantic analysis.  It produces, all being well, intermediate object
code: a intermediate representation (IR) of the source code.

Middle/Back:  This section takes the IR of the front end, and analyses it,
often using "data flow analysis."  The IR is then optimised: removes uselss
code, propogation of constant values, takes calculations out of loops. This
also creates IR code.  That IR code is then translated into the target
language: code generation.  It choses the right "target instruction" for
each IR instruction. Registers in the CPU are chosen, etc.

### 2 Intrepreters

Although both interpreters and compilers transform their source code into
something different, a compiler leaves the final execution of the program
to the operating system, ergo transforming it into a format the OS can run,
whereas an interpreters runs the transformed source code itself.

Intrepreters are generally slower than compilers because the interpreter
must analyse the code each time it is run, with the qualified and partial
exception of some formats, whereas that is already done in compiled code.

The term bytecode interpreter is used to designate transformed, and generally
more efficient, source code that an interpreter runs.  Abstract Syntax Trees
appear to be a datastructure that better represents the structure of the
program than bytecode; self-same statements need be parsed only once by the
interpreter, at at cost of traversing the tree.  Just-In-Time interpreters,
like Java, compile certain parts of the code at runtime.

### --- Lexical Analysis ---

### 3 Lexical Analysis: Intro

Tokenisation, as it's often called, is just the conversion of text strings,
via regular expressions usually, into various tokens, such as 'identifier',
'number', 'end statement' etc.

The lexer program does not do anything with the tokens it creates, not
even checking that opening parentheses are closed.  It leaves that stuff to
the parser.

The scanner part of a lexer looks at the characters to determine the type of
token it has to hand.  This is sometimes based on a finite state machine.
The tokenizer then categorises what the scanner has given it, often using
an evaluator, to grab a value from a token.

Lex is used for fast and simple grammar, but the most complex it becomes other
tools are used (perhaps like ragel?). Gcc, for example, uses hand-crafted
lexers.

### 4 Finite-state machine

This is a computer program that takes in a sequential number of symbols
as input and transitions to a fixed number of internal states, taking into
account the previous state, all depending on a certain set of accepted states.
State changes seem to depend on input and the current state. I.e. Input X
and state B will result in state c.

There are various 'actions' that are performed with the FSM. An entry action
happens when the state is first entered, an exit action as you can imagine,
an input action depending on the present state and input conditions, and
finally a transition action which happens /during/ a transition.

An acceptor/recognizer FSM produces a accept/reject output, not using any
'actions' at all.  If it took only the word 'nice' in, it would produce
an accept result if 'nice' was inputted, and a reject results if 'nica'
was inputted.

Transducers use actions and input to produce output.  They are often used
control applications.  A Moore machine only uses entry actions to control
something: it goes into one state and so a new action occurs.  A Mealy machine
uses input /and/ current state to control the application: the equivalent
of saying if this state AND this input.

Determinism is whether we know if the state has exactly one transition
(DFA) or if it has many (NFA: Non-deterministic finite automata).  This is
a useless concept when many FSMs have to work together.  An algorithm can
convert DFA to NFAs.

### 5 Preprocessor

This takes input and outputs in a format useful to another program,
i.e. macros in the c programming language being convereted into c code.
A lexical preprocessor is basically a lexer, as described above, meaning it
knows nothing of the underlying language, leaving all that to the parser.
M4 is another lexical parser used on the unix system.  The c preprocessor
can be used, for example, to define a set of javascript like syntax that
are converted to c syntax at compile time.

### --- Syntactic analysis ---

## 6 Parsing

Parsing is the act of getting a series of tokens to determine their grammatical
structure based on a 'formal' grammar.  Although the grammatical strucuture is
checked, type checking, for example, is not included in grammatical checking:
it is more concerned with values rather than correct composition.

After code is parsed as valid, its intention is the evaluated and the
appropriate action is then undertook, i.e. code generation.

Top down parsing uses a tree structure to determine, from left to right,
the correct grammatical structure for the input.  LL and recursive-descent
parser are examples.  Bottom-up parsing takes the smallest element in the
input and works its way up to the largest element. LR and Shift-Reduce is
a type of this kind.

A parser either reduces, that is it takes tokens and reduces them down to
a grammatical type such as addition or assignment or such, or shifts onto
the stack, that is it remembers the token for future analysis with the
subsequent tokens.

### 7 Lookhead

A lookhead type of parser looks at later input while analysing the current
input in order to make better decisions on the grammar, i.e. to look ahead
to find a multiplication symbol to ensure that is caculated before addition.
In terms of shift/reduce it shifts things before reducing them; it reduces
after it's looked at subsequent tokens.

### 8 Symbol table

A symbol table is used to map the tokens, such as function names or variables,
to information about such, such as their location and type.  A linker will
use this table to resolve any global indentifiers, such as the location of
an external library function.

### 9 Abstract Syntax Trees

AST differ from parse trees because they express the explict information,
such as if-else possibilies, using nodes, such as their being only two child
nodes, whereas parse trees will explicitly state this, apparently.

### 10 Context-free grammar

This is the method of breaking down a set of tokens from their smallest
part, combining such, into the full statement.  Block structure results from
this: that is, blocks in a if else statement, for example, make up the whole.

CFG have a set of variables and terminal characters. The expression
(S) has two terminal characters and one variable. Using rules this can be
expaneded to (((S)S)S) etc.


